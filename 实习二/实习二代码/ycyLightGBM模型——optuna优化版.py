import pandas as pd
import numpy as np
import os
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from optuna.samplers import TPESampler

# ============================
# 1. è®¾ç½®è·¯å¾„ä¸åŠ è½½æ•°æ®
# ============================
input_path = r"D:\å®ä¹ \åœ°ç†å¤§æ•°æ®å®ä¹ \å®ä¹ äºŒ\æ•°æ®\NYC_engineered1.csv"

print("ğŸš€ æ­£åœ¨åŠ è½½ç‰¹å¾å·¥ç¨‹åæ•°æ®...")
if not os.path.exists(input_path):
    raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ï¼š\n{input_path}")

df = pd.read_csv(input_path, low_memory=False)
print(f"âœ… åŠ è½½æˆåŠŸï¼å…± {len(df)} è¡Œ\n")

# ============================
# 2. å®šä¹‰ç‰¹å¾åˆ—ä¸ç›®æ ‡å˜é‡
# ============================
features = [
    'distance_km',
    'pickup_minute',
    'pickup_time',
    'is_weekend',
    'is_rest',
    'Temp.',
    'Visibility',
    'events_weather',
    'Precip',
    'pickup_cluster',
    'dropoff_cluster',
    'pickup_pca0',
    'pickup_pca1',
    'dropoff_pca0',
    'dropoff_pca1',
    'pickup_hour',
    'pickup_dayofweek',
    'pickup_month',
    'passenger_count',
    'pickup_longitude',
    'pickup_latitude',
    'dropoff_longitude',
    'dropoff_latitude',
    'ti_day',
    'ti_evening_peak',
    'ti_morning_peak',
    'ti_night'
]
target = 'trip_duration'

X = df[features]
y = np.log1p(df[target])  # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡

# ============================
# 3. åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆ60%:20%:20%ï¼‰
# ============================
print("ğŸ”¢ æ­£åœ¨åˆ’åˆ†æ•°æ®é›†ï¼ˆè®­ç»ƒ:éªŒè¯:æµ‹è¯• = 60%:20%:20%ï¼‰...")
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°ï¼š{len(X_train)}")
print(f"ğŸ“Š éªŒè¯é›†å¤§å°ï¼š{len(X_val)}")
print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°ï¼š{len(X_test)}\n")

# ============================
# 4. å®šä¹‰ Optuna ç›®æ ‡å‡½æ•°
# ============================
def objective(trial):
    # å®šä¹‰è¶…å‚æ•°ç©ºé—´
    params = {
        'objective': 'regression_l1',      # L1æŸå¤±ï¼ˆMAEï¼‰
        'metric': 'rmse',
        'n_estimators': trial.suggest_int('n_estimators', 200, 2000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 4, 8),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 0.9),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 0.9),
        'verbose': -1
        # 'objective': 'regression_l1',        # L2 æŸå¤±ï¼ˆRMSEï¼‰
        # 'metric': 'rmse',
        # 'n_estimators': 1000, #2000 1000
        # 'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
        # 'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
        # 'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),
        # 'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),
        # 'learning_rate': trial.suggest_categorical('learning_rate', [0.006, 0.008, 0.01, 0.014, 0.017, 0.02, 0.05]),
        # 'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17, 20, 50]),
        # 'num_leaves': trial.suggest_int('num_leaves', 1, 1000),
        # 'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),
        # 'cat_smooth': trial.suggest_int('cat_smooth', 1, 100),
        # 'random_state': 42,
        # 'verbose': -1
    }

    # åˆå§‹åŒ–æ¨¡å‹
    model = lgb.LGBMRegressor(**params,n_jobs=-1)

    # è®­ç»ƒæ¨¡å‹ï¼ˆå¯ç”¨æ—©åœï¼‰
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        callbacks=[
            lgb.early_stopping(stopping_rounds=20, verbose=False)
        ],
        categorical_feature=None
    )

    # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
    y_pred_log = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred_log))

    return rmse  # æœ€å°åŒ– RMSE

# ============================
# 5. å¯åŠ¨ Optuna ä¼˜åŒ–
# ============================
print("ğŸ¤– æ­£åœ¨ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°...")

# åˆ›å»ºç ”ç©¶å¯¹è±¡ï¼ˆstudyï¼‰
study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))

# å¼€å§‹ä¼˜åŒ–ï¼ˆæœ€å¤šè¿è¡Œ 50 æ¬¡è¯•éªŒï¼‰
study.optimize(objective, n_trials=50)

print("\nğŸ‰ ä¼˜åŒ–å®Œæˆï¼æœ€ä½³å‚æ•°ï¼š")
best_params = study.best_params
for k, v in best_params.items():
    print(f"  â€¢ {k}: {v}")

# ============================
# 6. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# ============================
print("\nğŸ‹ï¸ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")

final_model = lgb.LGBMRegressor(**best_params)

final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    callbacks=[
        lgb.early_stopping(stopping_rounds=20, verbose=False)
    ],
    categorical_feature=None
)

# ============================
# 7. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ & è¯„ä¼°æ€§èƒ½
# ============================
print("\nğŸ”® æ­£åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
y_pred_log = final_model.predict(X_test)
y_pred = np.expm1(y_pred_log)  # é€†å˜æ¢
y_true = np.expm1(y_test.values)

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"ğŸ† RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰ï¼š{rmse:.2f} ç§’")
print(f"ğŸ“Œ å¹³å‡è¡Œç¨‹æ—¶é•¿ï¼š{np.mean(y_true):.2f} ç§’ ({np.mean(y_true)/60:.2f} åˆ†é’Ÿ)")
print(f"ğŸ“Œ é¢„æµ‹å¹³å‡è¯¯å·®ï¼šÂ±{rmse:.2f} ç§’ ({rmse/60:.2f} åˆ†é’Ÿ)\n")

# ============================
# ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æŠ˜çº¿å›¾ï¼ˆæŠ½æ ·å±•ç¤ºï¼‰
# ============================
print("ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æŠ˜çº¿å›¾...")

# ä¸ºé¿å…å›¾å½¢è¿‡äºå¯†é›†ï¼ŒéšæœºæŠ½å– 200 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
np.random.seed(42)
sample_size = min(100, len(y_true))
indices = np.random.choice(len(y_true), size=sample_size, replace=False)

# æ’åºç´¢å¼•ä»¥ä¾¿æŠ˜çº¿å›¾æ›´æ¸…æ™°ï¼ˆæŒ‰çœŸå®å€¼æ’åºï¼‰
sorted_idx = np.argsort(y_true[indices])
x_plot = np.arange(sample_size)
y_true_plot = y_true[indices][sorted_idx]
y_pred_plot = y_pred[indices][sorted_idx]

plt.figure(figsize=(12, 6))
plt.plot(x_plot, y_true_plot, label='çœŸå®è¡Œç¨‹æ—¶é•¿', color='blue', linewidth=1.5, alpha=0.8)
plt.plot(x_plot, y_pred_plot, label='é¢„æµ‹è¡Œç¨‹æ—¶é•¿', color='red', linestyle='--', linewidth=1.5, alpha=0.8)
plt.title('é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯”ï¼ˆæµ‹è¯•é›†æŠ½æ ·ï¼‰', fontsize=16, weight='bold')
plt.xlabel('æ ·æœ¬åºå·ï¼ˆæŒ‰çœŸå®å€¼æ’åºï¼‰')
plt.ylabel('è¡Œç¨‹æ—¶é•¿ï¼ˆç§’ï¼‰')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.savefig("Prediction_vs_True_.png", dpi=300)
plt.show()

# ============================
# 8. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
# ============================
print("ğŸ“Š æ­£åœ¨ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾...")

feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': final_model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(
    data=feature_importance,
    x='Importance',
    y='Feature',
    hue='Feature',
    legend=False,
    palette="viridis"
)
plt.title("LightGBM Feature Importance (Optimized)", fontsize=16, weight="bold")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig("LightGBM Feature Importance_7",dpi=300)
plt.show()

# ============================
# 9. è¾“å‡ºæœ€é‡è¦çš„å‰5ä¸ªç‰¹å¾
# ============================
print("ğŸ”¥ æœ€é‡è¦çš„å‰5ä¸ªç‰¹å¾ï¼š")
print(feature_importance.head())

print("\nğŸ‰ Optuna + LightGBM æ¨¡å‹è®­ç»ƒã€ä¼˜åŒ–ã€é¢„æµ‹ä¸è¯„ä¼°å®Œæˆï¼")